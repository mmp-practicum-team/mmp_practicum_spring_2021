{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 практическое задание. Сегментация изображений.\n",
    "\n",
    "## Практикум на ЭВМ для 317 группы, весна 2021\n",
    "\n",
    "#### Фамилия, имя: \n",
    "\n",
    "Дата выдачи: 18 марта 00:00\n",
    "\n",
    "Мягкий дедлайн: 1 апреля 23:59 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В этом задании вы будете решать задачу сегментации людей на фотографии с помощью нейросетевых архитектур Unet и Linknet. \n",
    "\n",
    "Ссылка на данные: https://yadi.sk/d/-ug82uwhSuLYrA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Загрузка данных (1 балл)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для загрузки данных pytorch опирается на такую сущность, как **`Dataset`**.\n",
    "\n",
    "Этот абстрактный класс определен в `torch.utils.data.dataset`:\n",
    "\n",
    "```python\n",
    "class Dataset(object):\n",
    "    \"\"\"An abstract class representing a Dataset.\n",
    "\n",
    "    All other datasets should subclass it. All subclasses should override\n",
    "    ``__len__``, that provides the size of the dataset, and ``__getitem__``,\n",
    "    supporting integer indexing in range from 0 to len(self) exclusive.\n",
    "    \"\"\"\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def __len__(self):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def __add__(self, other):\n",
    "        return ConcatDataset([self, other])\n",
    "```\n",
    "При определении нового источника данных мы создаем наследника данного класса и реализуем методы `__getitem__` и `__len__`.\n",
    "\n",
    "Пример готового такого класса — `torchvision.datasets.ImageFolder`, который позволяет создать датасет на основе директории с imagenet-подобной структурой поддиректорий (`./train/{class}` и `./val/{class}`):\n",
    "\n",
    "```python\n",
    "imagenet = torchvision.datasets.ImageFolder('path/to/imagenet_root/')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Реализуйте класс PhotosDataset для выданных данных.\n",
    "\n",
    "**Внимание.** Возможно, стоит прочитать следующий пункт задания, чтобы реализация была удобной."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import numpy as np\n",
    "import numpy.testing as npt\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from torchvision.models import vgg13\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PhotosDataset(Dataset):\n",
    "    def __init__(self, images_dir, target_dir=None, transforms=None):\n",
    "        \"\"\"\n",
    "        Arguments\n",
    "        ---------\n",
    "        images_dir : str\n",
    "            Path to directory with images\n",
    "            \n",
    "        target_dir : str\n",
    "            Path to directory with masks.\n",
    "            Each mask corresponds to one image.\n",
    "            Corresponding mask and image have the same name, but different format.\n",
    "            \n",
    "        transforms : some collection\n",
    "            Sequence of transformations for images and masks. \n",
    "        \"\"\"\n",
    "        # your code here\n",
    "        \n",
    "    def __len__(self):\n",
    "        # your code here\n",
    "                   \n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Arguments\n",
    "        ---------\n",
    "        idx : int\n",
    "            Index of image and mask\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        (image, mask)\n",
    "        \"\"\"\n",
    "        # your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Модуль аугментации (3 балла)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Конструктор стандартного `ImageFolder`, принимают параметр `transform` (и иногда `target_transform`).\n",
    "\n",
    "Они служат для того, чтобы загружаемые изображения (обычно это `PIL.Image`) или таргеты преобразовывать в тензоры нужного вида.\n",
    "\n",
    "В `torchvision` входит модуль `transforms` для стандартных примеров таких преобразований. В `transforms` могут содержаться случайные преобразования, это самый простой путь для реализации аугментации данных.\n",
    "\n",
    "При определении кастомного трансформера помимо конструктора нужно реализовать лишь метод `__call__`:\n",
    "\n",
    "```python\n",
    "class HorizontalFlip(object):\n",
    "    def __init__(self, mode=0):\n",
    "        self.method = mode\n",
    "\n",
    "    def __call__(self, img):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            img (PIL.Image): Image to be flipped.\n",
    "\n",
    "        Returns:\n",
    "            PIL.Image: Randomly flipped image.\n",
    "        \"\"\"\n",
    "        if self.method:\n",
    "            return img.transpose(Image.FLIP_LEFT_RIGHT)\n",
    "        return img\n",
    "\n",
    "```\n",
    "\n",
    "С полным списком стандартных преобразований можно ознакомиться в http://pytorch.org/docs/master/torchvision/transforms.html."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В данной части вам предлагается самостояльно реализовать несколько кастомных трансформеров.\n",
    "Обратите внимание, что для некоторых трансформеров, необходимо преобразовывать и изображение, и маску, а для каких-то только изображение.\n",
    "\n",
    "Один из путей реализации:\n",
    "* реализовать декоратор, делающий любое преобразование случайным\n",
    "* реализовать декоратор, применяющий преобразование и к изображению, и к маске\n",
    "\n",
    "Список трансформеров, которые надо реализовать:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* случайное (с вероятностью p) горизонтальное отображение (flip) изображения \n",
    "* случайное (с веротностью p) вырезание фрагмента изображения (заданного или случайного размера)\n",
    "* случайное (с вероятностью p) изменение яркости изображения (на заданную или случайную величину)\n",
    "* случайное (с веротностью p) изменение фона изображения (на изображение из заданного списка изображений)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Добавьте случайные преобразования в ваши датасеты. Также, добавьте преобразование в tensor и нормализацию для изображения:\n",
    "\n",
    "```\n",
    "transforms.ToTensor()\n",
    "transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                     std=[0.229, 0.224, 0.225])\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = # your path\n",
    "\n",
    "train_dataset = PhotosDataset(\n",
    "    images_dir=DATA_PATH + 'train/',\n",
    "    target_dir=DATA_PATH + 'train_mask/',\n",
    "    transforms= # your code here\n",
    ")\n",
    "\n",
    "test_dataset = PhotosDataset(\n",
    "    images_dir=DATA_PATH + 'test/',\n",
    "    target_dir=DATA_PATH + 'test_mask/',\n",
    "    transforms= # your code here\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Функция для отображения изображения:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_idx_image(dataset, idx):\n",
    "    mask_img = dataset[idx]\n",
    "\n",
    "    image_to_show = mask_img[0].transpose(0, 1).transpose(1, 2).numpy()\n",
    "    image_to_show = (image_to_show * np.array([0.229, 0.224, 0.225])) + np.array([0.485, 0.456, 0.406])\n",
    "    image_to_show = np.clip(image_to_show, 0, 1)\n",
    "\n",
    "    mask_to_show = mask_img[1][0].numpy()\n",
    "\n",
    "    fig, ax = plt.subplots(1, 3)\n",
    "    fig.set_figwidth(7)\n",
    "    fig.set_figheight(3)\n",
    "\n",
    "\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.imshow(image_to_show)\n",
    "    plt.axis('off')\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.imshow(mask_to_show)\n",
    "    plt.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Отобразите несколько изображений и масок, на которых будет видна правильная работа вашего модуля аугментации данных."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Загрузчики"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "При обучении сети удобнее работать не с датасетами, а с загрузчиками. Загрузчики создаются на основе датасета и позволяют итерироваться по батчам из него.\n",
    "\n",
    "Обратите внимание на параметры DataLoader. При num_workers > 1, батчи готовятся (загружаются картинки, обрабатываются и т.д.) сразу в нескольких фоновых процессах. С помощью параметра shuffle можно подавать картинки на обучение в случайном порядке."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "test_data_loader = DataLoader(test_dataset, batch_size=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DiceLoss (1 балл)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Функция потерь реализовывается как и все стандартные нейронные модули в pytorch, через nn.Module. В ячейке ниже вам предлагается реализовать фукцию потерь dice (аналог меры Джаккарда).\n",
    "\n",
    "Результат dice loss определим как:\n",
    "\n",
    "$$\n",
    "1 - \\frac{2\\sum_{i, j}a_{ij}b_{ij}}{\\sum_{ij}(a_{ij} + b_{ij} + \\varepsilon)}\n",
    "$$\n",
    "\n",
    "где $a_{ij} \\in [0, 1]$ — предсказанная вероятность нахождения человека в пикселе изображения, $b_{ij}$ — разметка для пикселя изображения."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiceLoss(nn.Module):\n",
    "    def __init__(self, eps=1e-7, reduction=None, with_logits=True):\n",
    "        \"\"\"\n",
    "        Arguments\n",
    "        ---------\n",
    "        eps : float\n",
    "            eps in denominator\n",
    "        reduction : Optional[str] (None, 'mean' or 'sum')\n",
    "            specifies the reduction to apply to the output:\n",
    "            \n",
    "            None: no reduction will be applied\n",
    "            'mean': the sum of the output will be divided by the number of batches in the output\n",
    "            'sum':  the output will be summed. \n",
    "        with_logits : bool\n",
    "            If True, use additional sigmoid for inputs\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "        self.reduction = reduction\n",
    "        self.with_logits = with_logits\n",
    "        \n",
    "    def forward(self, logits, true_labels):\n",
    "        true_labels = true_labels.long()\n",
    "        \n",
    "        if self.with_logits:\n",
    "            logits = torch.sigmoid(logits)\n",
    "        \n",
    "        # your code here\n",
    "        \n",
    "        if self.reduction == 'sum':\n",
    "            loss_value = # your code\n",
    "        elif self.reduction == 'mean':\n",
    "            loss_value = # your code\n",
    "        elif self.reduction is None:\n",
    "            loss_value = # your code\n",
    "            \n",
    "        return loss_value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проверка реализации:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = torch.tensor([\n",
    "    [[0, 0.5], [0.5, 1]],\n",
    "    [[0.1, 0.1], [0, 0]],\n",
    "])\n",
    "\n",
    "target = torch.tensor([\n",
    "    [[0, 1], [1, 1]],\n",
    "    [[1, 0], [0, 1]],\n",
    "])\n",
    "\n",
    "losses = DiceLoss(with_logits=False, reduction=None, eps=1e-7)(logits, target)\n",
    "npt.assert_almost_equal(losses.numpy(), np.array([0.2, 0.90909]), decimal=4)\n",
    "\n",
    "loss = DiceLoss(with_logits=False, reduction='mean', eps=1e-7)(logits, target)\n",
    "npt.assert_almost_equal(float(loss.numpy()), 0.554545, decimal=4)\n",
    "\n",
    "loss = DiceLoss(with_logits=False, reduction='sum', eps=1e-7)(logits, target)\n",
    "npt.assert_almost_equal(float(loss.numpy()), 1.10909, decimal=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unet (2 балла)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для решения задачи сегментации будем использовать UNet (https://arxiv.org/pdf/1505.04597.pdf) с энкодером из первых блоков предобученного VGG13. Архитектура сети реализована за вас ниже.\n",
    "\n",
    "Энкодер:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VGG13Encoder(torch.nn.Module):\n",
    "    def __init__(self, num_blocks, pretrained=True):\n",
    "        super().__init__()\n",
    "        self.num_blocks = num_blocks\n",
    "        self.blocks = nn.ModuleList()\n",
    "        # Obtaining pretrained VGG model from torchvision.models and\n",
    "        # copying all layers except for max pooling.\n",
    "        feature_extractor = vgg13(pretrained=pretrained).features\n",
    "        for i in range(self.num_blocks):\n",
    "            self.blocks.append(\n",
    "                torch.nn.Sequential(*[feature_extractor[j]\n",
    "                                      for j in range(i * 5, i * 5 + 4)]))\n",
    "\n",
    "    def forward(self, x):\n",
    "        activations = []\n",
    "        for i in range(self.num_blocks):\n",
    "            x = self.blocks[i](x)\n",
    "            activations.append(x)\n",
    "            if i != self.num_blocks - 1:\n",
    "                x = torch.functional.F.max_pool2d(x, kernel_size=2, stride=2)\n",
    "        return activations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Декодер:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderBlock(torch.nn.Module):\n",
    "    def __init__(self, out_channels):\n",
    "        super().__init__()\n",
    "\n",
    "        self.upconv = torch.nn.Conv2d(\n",
    "            in_channels=out_channels * 2, out_channels=out_channels,\n",
    "            kernel_size=3, padding=1, dilation=1\n",
    "        )\n",
    "        self.conv1 = torch.nn.Conv2d(\n",
    "            in_channels=out_channels * 2, out_channels=out_channels,\n",
    "            kernel_size=3, padding=1, dilation=1\n",
    "        )\n",
    "        self.conv2 = torch.nn.Conv2d(\n",
    "            in_channels=out_channels, out_channels=out_channels,\n",
    "            kernel_size=3, padding=1, dilation=1\n",
    "        )\n",
    "        self.relu = nn.ReLU()\n",
    "    def forward(self, down, left):\n",
    "        x = torch.nn.functional.interpolate(down, scale_factor=2)\n",
    "        x = self.upconv(x)\n",
    "        x = self.relu(self.conv1(torch.cat([left, x], 1)))\n",
    "        x = self.relu(self.conv2(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, num_filters, num_blocks):\n",
    "        super().__init__()\n",
    "\n",
    "        for i in range(num_blocks):\n",
    "            self.add_module(f'block{num_blocks - i}', DecoderBlock(num_filters * 2**i))\n",
    "\n",
    "    def forward(self, acts):\n",
    "        up = acts[-1]\n",
    "        for i, left in enumerate(acts[-2::-1]):\n",
    "            up = self.__getattr__(f'block{i + 1}')(up, left)\n",
    "        return up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сеть Unet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNet(torch.nn.Module):\n",
    "    def __init__(self, num_classes=1, num_filters=64, num_blocks=4):\n",
    "        super().__init__()\n",
    "        self.encoder = VGG13Encoder(num_blocks=num_blocks)\n",
    "        self.decoder = Decoder(num_filters=64, num_blocks=num_blocks - 1)\n",
    "        self.final = torch.nn.Conv2d(\n",
    "            in_channels=num_filters, out_channels=num_classes, kernel_size=1\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        acts = self.encoder(x)\n",
    "        x = self.decoder(acts)\n",
    "        x = self.final(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Функция для оценивания качества сети:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_net(net, testloader, criterion, val_criterion, device='cpu'):\n",
    "    net = net.eval()\n",
    "\n",
    "    loss = 0.\n",
    "    correct = 0.\n",
    "    total = 0.\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for data in testloader:\n",
    "            images, labels = data\n",
    "            \n",
    "            images = images.to(device)\n",
    "            \n",
    "            outputs = net(images).to('cpu')\n",
    "            total += labels.size(0)\n",
    "            loss += float(criterion(outputs, labels).detach())\n",
    "            correct += float(val_criterion(outputs, labels).detach())\n",
    "    \n",
    "    mean_loss = loss / total\n",
    "    metric = correct / total\n",
    "    \n",
    "    return mean_loss, metric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь применим сеть на практике!\n",
    "\n",
    "Обучите сеть на обучающей выборке и протестируйте качество на тестовой выборке. \n",
    "\n",
    "Сначала при обучении используйте только кросс-энтропию (torch.nn.BCEWithLogitsLoss будет более удобна для бинарного случая). Зафиксируйте результат.\n",
    "\n",
    "Сравните между собой несколько стратегий оптимизации (только кросс-энтропия, только dice loss, сумма двух лоссов с весами). \n",
    "\n",
    "Для всех экспериментов отобразите графики функции потерь и качества за время обучения. Выведите получившиеся маски для нескольких изображений из датасета. Сделайте выводы."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LinkNet (2 балла)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Реализуйте архитектуру LinkNet с использованием энкодера, основанного на VGG13. Архитектура похожа на Unet, но вместо конкатенации слоёв используются skip-connections. Для реализации достаточно переписать структуру декодировщика из предыдущего пункта.\n",
    "\n",
    "https://arxiv.org/pdf/1707.03718.pdf\n",
    "\n",
    "Обучите сеть и проведите анализ согласно предыдущему пункту."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обучите LinkNet. Сравните LinkNet и Unet по качеству и скорости работы. Сделайте выводы. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Теоретические вопросы (1 балл)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Вопрос 1.\n",
    "* Сформулируйте два предположения о структуре входных данных, на которые опирается архитектура сверточных сетей. \n",
    "* Какие преимущества дает использование сверток в случае, если эти предположения верные? \n",
    "* Приведите пример входных данных, когда эти предположения не выполняются."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Ваш ответ в этой ячейке*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Вопрос 2\n",
    "Пусть дано множество 1-D входов ${x^{(i)} \\in \\mathbb{R}^{100}}$. \n",
    "\n",
    "Рассмотрим полносвязный слой ${f(\\cdot)}$, заданный как ${f(x^{(i)}) = \\sigma(Wx^{(i)})}$, где ${W}$ - матрица весов размерности ${1000\\times 100}$ и ${\\sigma(\\cdot)}$ поэлементая функция активации. \n",
    "\n",
    "Рассмотрим также сверточный слой ${g(\\cdot)}$ с десятью картами признаков: ${g(x^{(i)}) = \\sigma([z_1, z_2,...,z_{10}])}$, где ${z_j = x^{(i)}\\ast w_j}$ для некоторого ядра свертки ${w_j}$ с размером 3 и паддингом 1. Для ${f(\\cdot)}$ и ${g(\\cdot)}$ напишите: \n",
    "\n",
    "1. Размерность выходного пространства. \n",
    "2. Количество обучаемых параметров. \n",
    "3. Число операций при выполнении forward pass (при условии наивной реализации перемножения матриц и вычисления сверток)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Ваш ответ в этой ячейке*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Бонус: постобработка изображений (1 балл)\n",
    "\n",
    "В предложенной задаче на фотографиях необходимо сегментировать только центральное изображения. Артефакты, которые появляются на краях изображения, можно удалять с помощью постобработки (например, с помощью модуля https://scikit-image.org/docs/dev/api/skimage.morphology.html). \n",
    "\n",
    "Реализуйте какой-нибудь из методов постобработки, дающий прирост в качестве. Продемонстируйте несколько изображений, на которых постобработка будет оказывать влияние на результат сегментации."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Бонус: существенное улучшение качества (2 балла)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Придумайте какой-нибудь трюк, который существенно (с учётом бонусного пункта 1) повысит качество (которое должно быть и так достаточно высоко). Не разрешается использовать дополнительные данные или другие предобученные кодировщики кроме VGG13.\n",
    "\n",
    "Если вы что-то попробовали, но качество не повысилось, всё равно оформите этот пункт, даже за неудачные попытки могут быть начислены баллы."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Бонус: обучение с использованием сторонних фреймворков (1 балл)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Попробуйте обучить свою модель с использованием одного из следующих фреймворков: catalyst, pytorch-lightning, kekas."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
